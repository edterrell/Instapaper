{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e47113-559c-4d5d-a8a2-078e4e4b4671",
   "metadata": {},
   "source": [
    "### Instapaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8078af57-9028-45e8-8312-1417b5dc6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import time\n",
    "#import sys\n",
    "#sys.executable\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb8bd615-9e9e-4cc1-9924-8bb683f7a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get username and password and store them in variables\n",
    "from pathlib import Path\n",
    "base_path = Path.home() / \"bin\" / \"passwords\"\n",
    "with open(base_path / \"instapaper_username.txt\", \"r\") as f:\n",
    "    username = f.read().strip()\n",
    "with open(base_path / \"instapaper_password.txt\", \"r\") as f:\n",
    "    password = f.read().strip()\n",
    "\n",
    "#print(username, password)  # Just for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "427c9852-d4ea-4fcf-a53f-8fee277cb86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "# Start Safari (or Chrome)\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "driver.get(\"https://www.instapaper.com/user/login\")\n",
    "username_field = driver.find_element(By.ID, \"username\")\n",
    "password_field = driver.find_element(By.ID, \"password\")\n",
    "\n",
    "# Fill in login form\n",
    "username_field.send_keys(username)\n",
    "time.sleep(1)\n",
    "password_field.send_keys(password)\n",
    "password_field.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for redirect\n",
    "time.sleep(3)\n",
    "\n",
    "def parse_url(url):\n",
    "    all_clean_links = []\n",
    "    try:\n",
    "        # Go to archive and grab links\n",
    "        driver.get(url)\n",
    "        # Instead of time.sleep(2) we use WebDriverWait\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'article_text_content'))\n",
    "        )\n",
    "        html = driver.page_source\n",
    "        \n",
    "        # Now we are in the beautiful soup world\n",
    "        soup = bs(html, \"html.parser\")\n",
    "        links = [a['href'] for a in soup.find_all('a', href=True)]  \n",
    "        clean_links = [link for link in links if link.startswith(\"http\")]\n",
    "        all_clean_links.extend(clean_links)\n",
    "        \n",
    "        # Check for \"next page\" link\n",
    "        next_soup = None\n",
    "        next_link_tag = soup.find('a', class_='paginate_older')\n",
    "        if next_link_tag:\n",
    "            next_url = next_link_tag['href']\n",
    "            #if not next_url.startswith(\"http\"):\n",
    "                #next_url = \"https://www.instapaper.com\" + next_url\n",
    "                \n",
    "            # Follow next page and repeat\n",
    "            driver.get(next_url)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CLASS_NAME, 'article_text_content'))\n",
    "            )\n",
    "            next_html = driver.page_source\n",
    "            next_soup = bs(next_html, \"html.parser\")\n",
    "            next_links = [a['href'] for a in next_soup.find_all('a', href=True)]\n",
    "            clean_next_links = [link for link in next_links if link.startswith(\"http\")]\n",
    "            all_clean_links.extend(clean_next_links)      \n",
    "                            \n",
    "    except TimeoutException:\n",
    "        print(\"Timeout while loading archive page.\")\n",
    "    \n",
    "    \n",
    "    return all_clean_links, soup, next_soup\n",
    "\n",
    "# assign the function's return variables to be used out of scope\n",
    "url = \"https://www.instapaper.com/archive\"\n",
    "all_clean_links, soup, next_soup = parse_url(url)   \n",
    "\n",
    "driver.quit()\n",
    "# print(len(all_clean_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a30a5de8-fc97-4972-b517-318f4388479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store clickable_links.html for web viewing \n",
    "# Filter out links containing \"instapaper\" or \"ads/click\"\n",
    "urls = [\n",
    "    url for url in all_clean_links\n",
    "    if 'instapaper' not in url and 'ads/click' not in url\n",
    "]\n",
    "\n",
    "# Create an HTML file with clickable links\n",
    "with open(\"clickable_links.html\", \"w\") as outfile:\n",
    "    outfile.write(\"<!DOCTYPE html>\\n<html>\\n<body>\\n<h2>Clickable Links</h2>\\n<ul>\\n\")\n",
    "    for url in urls:\n",
    "        outfile.write(f'  <li><a href=\"{url}\" target=\"_blank\">{url}</a></li>\\n')\n",
    "    outfile.write(\"</ul>\\n</body>\\n</html>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "682108c5-e6d0-4803-93f1-b7e130ce68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create titles,previews,urls \n",
    "def extract_article_data(soup):\n",
    "    titles = [title.text.strip().replace('\\xa0', ' ') for title in soup.find_all('a', class_='article_title')]\n",
    "    previews = [prev.text.strip().replace('\\xa0', ' ') for prev in soup.find_all('div', class_='article_preview')]\n",
    "    url_links = [link['href'] for link in soup.find_all('a', class_='js_domain_linkout')]\n",
    "    return titles, previews, url_links\n",
    "\n",
    "titles, previews, url_links = extract_article_data(soup)\n",
    "\n",
    "if next_soup:\n",
    "    titles2, previews2, url_links2 = extract_article_data(next_soup)\n",
    "\n",
    "titles = titles + titles2\n",
    "previews = previews + previews2\n",
    "url_links = url_links + url_links2\n",
    "\n",
    "df = pd.DataFrame({'Title':titles,'Link':url_links,'Preview':previews})\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81049577-df8d-4ce6-bd94-9272b8097290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas to csv file\n",
    "df.to_csv('instapaper_articles.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce88b9d-9beb-42e7-9a7a-35abf2242313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store article_listing.html file for web viewing \n",
    "# Save df_display for later use\n",
    "df_display = df.copy()\n",
    "\n",
    "def shorten_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc.replace(\"www.\", \"\")\n",
    "    return f'<a href=\"{url}\" target=\"_blank\">{domain}</a>'\n",
    "\n",
    "df[\"Link\"] = df[\"Link\"].apply(shorten_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "959f942e-d106-4523-8953-5ebcd856b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to HTML\n",
    "html_table = df.to_html(escape=False, index=False)\n",
    "\n",
    "html_full = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>Exported Table</title>\n",
    "</head>\n",
    "<body>\n",
    "{html_table}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open(\"article_listing.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html_full)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
